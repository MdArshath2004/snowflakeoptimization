import streamlit as st
import pandas as pd
import snowflake.connector
import google.generativeai as genai
from typing import TypedDict, List, Dict
from langgraph.graph import StateGraph, END # Required for Agentic Workflow
from langchain_google_genai import ChatGoogleGenerativeAI
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import json
import time
import os
# dotenv import REMOVED

# ==================== CONFIGURATION & SECRETS (HARDCODED) ====================

# 1. Gemini API Key (HARDCODED)
GEMINI_API_KEY = "AIzaSyCRbP8fryI0cCfmPoLh9aYQxekxINdg5iQ"Â 

# 2. Snowflake Configuration (HARDCODED to specific paths)
SNOWFLAKE_CONFIG = {
Â  Â  'account': 'AUYWMHB-UN24606',
Â  Â  'user': 'MOHAMEDARSHATH3',
Â  Â  'password': 'Arshath@302004',Â 
Â  Â  'database': 'MOHAMED_ARSHATH_PROJECT_DB', # FIXED: Target Database
Â  Â  'schema': 'BASELINE_DATA',Â  Â  Â  Â  Â  Â  Â  Â  # FIXED: Target Schema
Â  Â  'warehouse': 'COMPUTE_WH'
}

# Target table path components
TARGET_DB = SNOWFLAKE_CONFIG['database']
TARGET_SCHEMA = SNOWFLAKE_CONFIG['schema']
TARGET_TABLE_NAME = "OPTIMIZER_BASELINE_DATA"Â 
TARGET_TABLE_FULL = f"{TARGET_DB}.{TARGET_SCHEMA}.{TARGET_TABLE_NAME}"


# Application constraints and resources
USABLE_WAREHOUSES = [
Â  Â  "COMPUTE_WH",Â 
Â  Â  "WH_SMALL_SYNTHETIC",Â 
Â  Â  "WH_XSMALL_SYNTHETIC",Â 
Â  Â  "WH_LARGE_SYNTHETIC",Â 
Â  Â  "WH_MEDIUM_SYNTHETIC",Â 
Â  Â  "WH_XLARGE_SYNTHETIC",Â 
Â  Â  "SNOWFLAKE_LEARNING_WH"
]

# --- Deployment Checks ---
if not GEMINI_API_KEY:
Â  Â  st.error("FATAL: GEMINI_API_KEY is missing. Check the code's hardcoded value.")
Â  Â  st.stop()

if not SNOWFLAKE_CONFIG.get('password') or not SNOWFLAKE_CONFIG.get('account'):
Â  Â  st.error("FATAL: Essential Snowflake configuration is missing. Cannot connect to Snowflake.")
Â  Â  st.stop()


# ==============================================================================
# STREAMLIT SETUP AND AGENT STATE DEFINITION
# ==============================================================================

st.set_page_config(
Â  Â  page_title="LLM Warehouse Optimizer",
Â  Â  page_icon="ðŸ¤–",
Â  Â  layout="wide"
)

st.markdown("""
<style>
Â  Â  .main-header {
Â  Â  Â  Â  background: linear-gradient(90deg, #1e3a8a 0%, #3b82f6 100%);
Â  Â  Â  Â  padding: 2rem;
Â  Â  Â  Â  border-radius: 10px;
Â  Â  Â  Â  color: white;
Â  Â  Â  Â  text-align: center;
Â  Â  Â  Â  margin-bottom: 2rem;
Â  Â  }
Â  Â  .stButton>button {
Â  Â  Â  Â  width: 100%;
Â  Â  }
</style>
""", unsafe_allow_html=True)

class AgentState(TypedDict):
Â  Â  """The shared memory for the LangGraph workflow."""
Â  Â  query_id: str
Â  Â  query_text: str
Â  Â  original_warehouse: str
Â  Â  query_type: str
Â  Â  original_execution_time_ms: float
Â  Â  original_bytes_scanned: float
Â  Â  original_efficiency_score: float
Â  Â  original_estimated_cost: floatÂ 
Â  Â  recommended_warehouse: str
Â  Â  optimization_reason: str
Â  Â  optimized_query: str
Â  Â  optimization_suggestions: List[str]
Â  Â  execution_result: str
Â  Â  new_execution_time_ms: float
Â  Â  actual_warehouse_used: str
Â  Â  execution_status: str
Â  Â  error_message: str

# ==================== HELPER FUNCTION: SAFE CONNECTION MANAGER ====================
def _create_snowflake_connection_safe(config: Dict):
Â  Â  """Establishes a Snowflake connection and automatically resumes the warehouse."""
Â  Â  try:
Â  Â  Â  Â  conn = snowflake.connector.connect(**config)
Â  Â  Â  Â  warehouse_name = config.get('warehouse', 'COMPUTE_WH')
Â  Â  Â  Â  cursor = conn.cursor()
Â  Â  Â  Â  cursor.execute(f"ALTER WAREHOUSE {warehouse_name} RESUME IF SUSPENDED;")
Â  Â  Â  Â  cursor.close()
Â  Â  Â  Â  return conn
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"âŒ Snowflake Connection Failed: {e}")
Â  Â  Â  Â  raise e

# ==============================================================================
# WAREHOUSE OPTIMIZER AGENT CLASS (LANGGRAPH NODES)
# ==============================================================================

class WarehouseOptimizerAgent:
Â  Â  def __init__(self, gemini_api_key: str, sf_config: dict):
Â  Â  Â  Â  self.genai_model = ChatGoogleGenerativeAI(
Â  Â  Â  Â  Â  Â  model="gemini-2.5-flash",Â 
Â  Â  Â  Â  Â  Â  google_api_key=gemini_api_key,
Â  Â  Â  Â  Â  Â  temperature=0.0
Â  Â  Â  Â  )
Â  Â  Â  Â  self.sf_config = sf_config
Â  Â  Â  Â  self.workflow = self.build_workflow()
Â  Â Â 
Â  Â  def get_snowflake_connection(self, warehouse=None):
Â  Â  Â  Â  """Create Snowflake connection with optional dynamic warehouse override."""
Â  Â  Â  Â  return snowflake.connector.connect(
Â  Â  Â  Â  Â  Â  account=self.sf_config['account'],
Â  Â  Â  Â  Â  Â  user=self.sf_config['user'],
Â  Â  Â  Â  Â  Â  password=self.sf_config['password'],
Â  Â  Â  Â  Â  Â  database=self.sf_config['database'],
Â  Â  Â  Â  Â  Â  schema=self.sf_config['schema'],
Â  Â  Â  Â  Â  Â  warehouse=warehouse or self.sf_config['warehouse']Â 
Â  Â  Â  Â  )
Â  Â Â 
Â  Â  # ----------------- LANGGRAPH NODE 1: RECOMMEND WAREHOUSE -----------------
Â  Â  def recommend_warehouse(self, state: AgentState) -> AgentState:
Â  Â  Â  Â  """Step 1: LLM recommends optimal warehouse based on query history."""
Â  Â  Â  Â  available_wh_list = ", ".join(USABLE_WAREHOUSES)
Â  Â  Â  Â Â 
Â  Â  Â  Â  prompt = f"""You are a Snowflake warehouse optimization expert. Analyze this query and recommend the best warehouse.

Available Warehouses (You MUST choose one of these exact names): {available_wh_list}

Current Query Details:
- Query ID: {state['query_id']}
- Current Warehouse: {state['original_warehouse']}
- Query Type: {state['query_type']}
- Execution Time: {state['original_execution_time_ms']} ms
- Bytes Scanned: {state['original_bytes_scanned']} bytes
- Efficiency Score: {state['original_efficiency_score']}
- Current Cost: ${state['original_estimated_cost']}

Query:
{state['query_text']}

Analyze complexity (SELECT, JOIN, GROUP BY, aggregations, subqueries, data volume) and recommend:
1. Best warehouse name (must be one of: {available_wh_list})
2. Clear reasoning in 2 to 3 lines

Guidelines:
- Match warehouse size to query complexity to balance speed and cost.
- Simple SELECT with filters â†’ X-SMALL or SMALL warehouses (WH_XSMALL_SYNTHETIC, SNOWFLAKE_LEARNING_WH)
- Complex joins, multiple aggregations â†’ LARGE warehouses (WH_LARGE_SYNTHETIC, WH_XLARGE_SYNTHETIC)

Response as JSON:
{{
Â  Â  "recommended_warehouse": "WAREHOUSE_NAME_FROM_LIST",
Â  Â  "reasoning": "detailed explanation"
}}"""
Â  Â  Â  Â Â 
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  response = self.genai_model.invoke(prompt)
Â  Â  Â  Â  Â  Â  result_text = response.content.strip()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if "```json" in result_text:
Â  Â  Â  Â  Â  Â  Â  Â  result_text = result_text.split("```json")[1].split("```")[0].strip()
Â  Â  Â  Â  Â  Â  elif "```" in result_text:
Â  Â  Â  Â  Â  Â  Â  Â  result_text = result_text.split("```")[1].split("```")[0].strip()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  result = json.loads(result_text)
Â  Â  Â  Â  Â  Â  recommended_wh = result['recommended_warehouse'].upper().strip()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if recommended_wh not in [w.upper() for w in USABLE_WAREHOUSES]:
Â  Â  Â  Â  Â  Â  Â  Â  state['recommended_warehouse'] = state['original_warehouse']
Â  Â  Â  Â  Â  Â  Â  Â  state['optimization_reason'] = f"LLM suggested an invalid warehouse: {recommended_wh}. Using original warehouse."
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  state['recommended_warehouse'] = recommended_wh
Â  Â  Â  Â  Â  Â  Â  Â  state['optimization_reason'] = result['reasoning']

Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  state['recommended_warehouse'] = state['original_warehouse']
Â  Â  Â  Â  Â  Â  state['optimization_reason'] = f"Error in recommendation: {str(e)}. Using original warehouse."
Â  Â  Â  Â  Â  Â  state['error_message'] = str(e)
Â  Â  Â  Â Â 
Â  Â  Â  Â  return state
Â  Â Â 
Â  Â  # ----------------- LANGGRAPH NODE 2: OPTIMIZE QUERY -----------------
Â  Â  def optimize_query(self, state: AgentState) -> AgentState:
Â  Â  Â  Â  """Step 2: LLM optimizes SQL query based on best practices."""
Â  Â  Â  Â Â 
Â  Â  Â  Â  prompt = f"""
You are an expert Snowflake SQL Performance Engineer. Analyze and optimize the following query for maximum performance while maintaining identical results.
... (Optimization prompt omitted for brevity) ...
Response as JSON:
{{
Â  Â  "optimized_query": "REWRITTEN SQL QUERY HERE",
Â  Â  "optimizations": ["List of specific changes made (e.g., Removed SELECT * in line 5)."]
}}"""
Â  Â  Â  Â Â 
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  response = self.genai_model.invoke(prompt)
Â  Â  Â  Â  Â  Â  result_text = response.content.strip()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if "```json" in result_text: result_text = result_text.split("```json")[1].split("```")[0].strip()
Â  Â  Â  Â  Â  Â  elif "```" in result_text: result_text = result_text.split("```")[1].split("```")[0].strip()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  result = json.loads(result_text)
Â  Â  Â  Â  Â  Â  state['optimized_query'] = result['optimized_query']
Â  Â  Â  Â  Â  Â  state['optimization_suggestions'] = result['optimizations']
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  state['optimized_query'] = state['query_text']Â 
Â  Â  Â  Â  Â  Â  state['optimization_suggestions'] = [f"Using original query due to optimization error: {str(e)}"]
Â  Â  Â  Â  Â  Â  state['error_message'] = str(e)
Â  Â  Â  Â Â 
Â  Â  Â  Â  return state
Â  Â Â 
Â  Â  # ----------------- LANGGRAPH NODE 3: EXECUTE IN SNOWFLAKE (ACCURATE TIMING) -----------------
Â  Â  def execute_in_snowflake(self, state: AgentState) -> AgentState:
Â  Â  Â  Â  """Step 3: Execute optimized query and retrieve official execution time."""
Â  Â  Â  Â Â 
Â  Â  Â  Â  conn = None
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  conn = self.get_snowflake_connection(warehouse=state['recommended_warehouse'])
Â  Â  Â  Â  Â  Â  cursor = conn.cursor()
Â  Â Â 
Â  Â  Â  Â  Â  Â  # Get actual warehouse name for logging
Â  Â  Â  Â  Â  Â  cursor.execute("SELECT CURRENT_WAREHOUSE()")
Â  Â  Â  Â  Â  Â  state['actual_warehouse_used'] = cursor.fetchone()[0]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Execute the optimized query
Â  Â  Â  Â  Â  Â  start_time = time.time()
Â  Â  Â  Â  Â  Â  cursor.execute(state['optimized_query'])
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  execution_duration_ms = (time.time() - start_time) * 1000Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  state['new_execution_time_ms'] = execution_duration_ms
Â  Â  Â  Â  Â  Â  state['execution_status'] = 'SUCCESS'
Â  Â  Â  Â  Â  Â  state['execution_result'] = f"Query executed successfully on {state['actual_warehouse_used']}."
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  cursor.close()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  state['execution_status'] = 'FAILED'
Â  Â  Â  Â  Â  Â  state['execution_result'] = f"Execution failed: {str(e)}"
Â  Â  Â  Â  Â  Â  state['error_message'] = str(e)
Â  Â  Â  Â  Â  Â  state['new_execution_time_ms'] = 0
Â  Â  Â  Â  Â  Â  state['actual_warehouse_used'] = 'N/A'
Â  Â  Â  Â  finally:
Â  Â  Â  Â  Â  Â  if conn:
Â  Â  Â  Â  Â  Â  Â  Â  conn.close()
Â  Â  Â  Â Â 
Â  Â  Â  Â  return state
Â  Â Â 
Â  Â  def build_workflow(self):
Â  Â  Â  Â  """Defines the sequential LangGraph pipeline."""
Â  Â  Â  Â  workflow = StateGraph(AgentState)
Â  Â  Â  Â Â 
Â  Â  Â  Â  workflow.add_node("recommend_warehouse", self.recommend_warehouse)
Â  Â  Â  Â  workflow.add_node("optimize_query", self.optimize_query)
Â  Â  Â  Â  workflow.add_node("execute_query", self.execute_in_snowflake)
Â  Â  Â  Â Â 
Â  Â  Â  Â  workflow.set_entry_point("recommend_warehouse")
Â  Â  Â  Â  workflow.add_edge("recommend_warehouse", "optimize_query")
Â  Â  Â  Â  workflow.add_edge("optimize_query", "execute_query")
Â  Â  Â  Â  workflow.add_edge("execute_query", END)
Â  Â  Â  Â Â 
Â  Â  Â  Â  return workflow.compile()
Â  Â Â 
Â  Â  def process_query_record(self, record: dict) -> AgentState:
Â  Â  Â  Â  """Initializes the state and runs the compiled LangGraph workflow."""
Â  Â  Â  Â  initial_state = AgentState(
Â  Â  Â  Â  Â  Â  query_id=str(record['QUERY_ID']), query_text=record['QUERY_TEXT'], original_warehouse=record['ORIGINAL_WAREHOUSE'], query_type=record['QUERY_TYPE'],
Â  Â  Â  Â  Â  Â  original_execution_time_ms=float(record['ORIGINAL_EXECUTION_TIME_MS']), original_bytes_scanned=float(record['ORIGINAL_BYTES_SCANNED']),
Â  Â  Â  Â  Â  Â  original_efficiency_score=float(record['ORIGINAL_EFFICIENCY_SCORE']), original_estimated_cost=float(record['ORIGINAL_ESTIMATED_COST']),
Â  Â  Â  Â  Â  Â  recommended_warehouse='', optimization_reason='', optimized_query='',
Â  Â  Â  Â  Â  Â  optimization_suggestions=[], execution_result='', new_execution_time_ms=0,
Â  Â  Â  Â  Â  Â  actual_warehouse_used='', execution_status='PENDING', error_message=''
Â  Â  Â  Â  )
Â  Â  Â  Â  return self.workflow.invoke(initial_state)

def set_page(page_name):
Â  Â  """Updates session state to control which page function is rendered."""
Â  Â  st.session_state.page = page_name

# ----------------- PAGE 1: DATA LOADING AND SELECTION -----------------
def page_1_load_data():
Â  Â Â 
Â  Â  # Define target table path
Â  Â  TARGET_DB = SNOWFLAKE_CONFIG.get('database')
Â  Â  TARGET_SCHEMA = SNOWFLAKE_CONFIG.get('schema')
Â  Â  TARGET_TABLE_FULL = f"{TARGET_DB}.{TARGET_SCHEMA}.OPTIMIZER_BASELINE_DATA"
Â  Â Â 
Â  Â  st.header("ðŸ“Š Load Data and Select Records")
Â  Â Â 
Â  Â  col1, col2 = st.columns([3, 1])
Â  Â  with col1:
Â  Â  Â  Â  st.info(f"Source Table: **{TARGET_TABLE_FULL}**")
Â  Â  with col2:
Â  Â  Â  Â  if st.button("ðŸ”„ Load All Records", type="primary"):
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Connecting to Snowflake and loading records..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  conn = _create_snowflake_connection_safe(SNOWFLAKE_CONFIG)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Target the verified baseline table and alias ORIGINAL_COST
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  query = f"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  SELECTÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  QUERY_ID, QUERY_TEXT, ORIGINAL_WAREHOUSE, QUERY_TYPE,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ORIGINAL_EXECUTION_TIME_MS, ORIGINAL_BYTES_SCANNED,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ORIGINAL_EFFICIENCY_SCORE,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ORIGINAL_COST AS ORIGINAL_ESTIMATED_COSTÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  FROM {TARGET_TABLE_FULL}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY Random();
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df = pd.read_sql(query, conn)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  conn.close()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Ensure column names are uppercase to match AgentState
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df.columns = df.columns.str.upper()Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.all_records = df
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"âœ… Loaded {len(df)} records successfully! Ready for analysis.")
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ Error loading data. Ensure table {TARGET_TABLE_FULL} exists: {str(e)}")
Â  Â  Â  Â  Â  Â  Â  Â  st.stop()

Â  Â  if st.session_state.all_records is not None:
Â  Â  Â  Â  df = st.session_state.all_records
Â  Â  Â  Â  st.subheader(f"ðŸ“‹ All Records ({len(df)} total)")
Â  Â  Â  Â  st.dataframe(df, use_container_width=True, height=300)
Â  Â  Â  Â  st.markdown("---")

Â  Â  Â  Â  st.subheader("ðŸŽ¯ Select Records for Analysis")
Â  Â  Â  Â  num_records = st.slider(
Â  Â  Â  Â  Â  Â  "How many records should the LLM analyze?",
Â  Â  Â  Â  Â  Â  min_value=1,
Â  Â  Â  Â  Â  Â  max_value=len(df),
Â  Â  Â  Â  Â  Â  value=min(5, len(df)),
Â  Â  Â  Â  Â  Â  key="num_records_slider",
Â  Â  Â  Â  Â  Â  help="Select number of records for optimization."
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.session_state.selected_records = df.head(num_records)
Â  Â  Â  Â  st.info(f"ðŸ“Œ Selected {num_records} records.")
Â  Â  Â  Â  st.dataframe(st.session_state.selected_records, use_container_width=True, height=200)

Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  if st.button("âž¡ï¸ Proceed to Analysis and optimization", type="secondary", use_container_width=True):
Â  Â  Â  Â  Â  Â  if len(st.session_state.selected_records) > 0:
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.execution_results = []Â 
Â  Â  Â  Â  Â  Â  Â  Â  set_page("analysis")
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Please load and select at least one record.")

# ----------------- PAGE 2: LLM ANALYSIS AND OPTIMIZATION -----------------
def page_2_analysis():
Â  Â  st.header("ðŸ¤–Â  LLM Analysis and Optimization")
Â  Â Â 
Â  Â  if st.button("â¬…ï¸ Back to Data Selection", key="back_to_page1"):
Â  Â  Â  Â  set_page("data_load")
Â  Â  Â  Â  st.rerun()

Â  Â  num_records = len(st.session_state.selected_records)
Â  Â  st.info(f"Analyzing {num_records} selected queries...")

Â  Â  col1, col2 = st.columns(2)
Â  Â Â 
Â  Â  with col1:
Â  Â  Â  Â  if st.button("ðŸ§  Run LLM Analysis & Get Recommendations", type="primary", use_container_width=True):
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  agent = WarehouseOptimizerAgent(GEMINI_API_KEY, SNOWFLAKE_CONFIG)
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.recommendations = []
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  progress_bar = st.progress(0)
Â  Â  Â  Â  Â  Â  Â  Â  status_text = st.empty()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  for idx, row in st.session_state.selected_records.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  status_text.text(f"ðŸ” Analyzing Query {idx + 1}/{num_records}: {row['QUERY_ID']}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- Convert Pandas row to AgentState compatible dict ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  record_dict = row.to_dict()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for key in ['ORIGINAL_EXECUTION_TIME_MS', 'ORIGINAL_BYTES_SCANNED', 'ORIGINAL_EFFICIENCY_SCORE', 'ORIGINAL_ESTIMATED_COST']:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  record_dict[key] = float(record_dict.get(key, 0)) if pd.notna(record_dict.get(key)) else 0.0

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  initial_state = AgentState(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  query_id=str(record_dict['QUERY_ID']), query_text=record_dict['QUERY_TEXT'], original_warehouse=record_dict['ORIGINAL_WAREHOUSE'], query_type=record_dict['QUERY_TYPE'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  original_execution_time_ms=record_dict['ORIGINAL_EXECUTION_TIME_MS'], original_bytes_scanned=record_dict['ORIGINAL_BYTES_SCANNED'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  original_efficiency_score=record_dict['ORIGINAL_EFFICIENCY_SCORE'], original_estimated_cost=record_dict['ORIGINAL_ESTIMATED_COST'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  recommended_warehouse='', optimization_reason='', optimized_query='',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  optimization_suggestions=[], execution_result='', new_execution_time_ms=0,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  actual_warehouse_used='', execution_status='PENDING', error_message=''
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Execute LangGraph: Recommend -> Optimize
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  state = agent.recommend_warehouse(initial_state)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  state = agent.optimize_query(state)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.recommendations.append(state)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_bar.progress((idx + 1) / num_records)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  status_text.text("âœ… Analysis complete!")
Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"ðŸŽ‰ Generated recommendations for {len(st.session_state.recommendations)} queries!")
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ Error during analysis: {str(e)}")
Â  Â Â 
Â  Â  with col2:
Â  Â  Â  Â  if st.button("ðŸ—‘ï¸ Clear Recommendations", use_container_width=True, key="clear_recs"):
Â  Â  Â  Â  Â  Â  st.session_state.recommendations = []
Â  Â  Â  Â  Â  Â  st.session_state.execution_results = []
Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  # Display recommendations
Â  Â  if st.session_state.recommendations:
Â  Â  Â  Â  st.subheader("ðŸ’¡ LLM Recommendations Details")
Â  Â  Â  Â Â 
Â  Â  Â  Â  for rec in st.session_state.recommendations:
Â  Â  Â  Â  Â  Â  with st.expander(f"ðŸ” Query: {rec['query_id']} | Recommended WH: {rec['recommended_warehouse']}", expanded=False):
Â  Â  Â  Â  Â  Â  Â  Â  colA, colB = st.columns(2)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with colA:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### ðŸ“Š Original Performance")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Warehouse", rec['original_warehouse'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Execution Time", f"{rec['original_execution_time_ms']:.0f} ms")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Cost", f"${rec['original_estimated_cost']:.4f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Efficiency Score", f"{rec['original_efficiency_score']:.2f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.code(rec['query_text'], language="sql")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with colB:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### ðŸš€ LLM Recommendation")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Recommended WH", rec['recommended_warehouse'],Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  delta=f"Change from {rec['original_warehouse']}" if rec['recommended_warehouse'] != rec['original_warehouse'] else "Same")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Reasoning:**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info(rec['optimization_reason'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if rec['optimization_suggestions']:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Query Optimizations:**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for opt in rec['optimization_suggestions']:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"âœ“ {opt}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Optimized Query:**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.code(rec['optimized_query'], language="sql")

Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  st.header("âš¡Â  Execution Validation")
Â  Â  Â  Â  if st.button("ðŸš€ Proceed to Validation (Step 4)", type="primary", use_container_width=True):
Â  Â  Â  Â  Â  Â  set_page("validation")
Â  Â  Â  Â  Â  Â  st.rerun()

# ----------------- PAGE 3: VALIDATION AND RESULTS -----------------
def page_3_validation_and_results():
Â  Â  st.header("ðŸ“ˆ Execution Validation and Results")
Â  Â Â 
Â  Â  if st.button("â¬…ï¸ Back to Analysis", key="back_to_page2"):
Â  Â  Â  Â  set_page("analysis")
Â  Â  Â  Â  st.rerun()

Â  Â  st.warning("âš ï¸ Executing validation runs the optimized queries on the recommended warehouses in Snowflake to get official time.")
Â  Â Â 
Â  Â  if not st.session_state.execution_results:
Â  Â  Â  Â  if st.button("âš¡ Run Execution Validation", type="primary", use_container_width=True):
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  agent = WarehouseOptimizerAgent(GEMINI_API_KEY, SNOWFLAKE_CONFIG)
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.execution_results = []
Â  Â  Â  Â  Â  Â  Â  Â  progress_bar = st.progress(0)
Â  Â  Â  Â  Â  Â  Â  Â  status_text = st.empty()
Â  Â  Â  Â  Â  Â  Â  Â  total_recs = len(st.session_state.recommendations)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  for idx, rec in enumerate(st.session_state.recommendations):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  status_text.text(f"âš¡ Executing Query {idx + 1}/{total_recs}: {rec['query_id']}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # *** CLONING THE RECORD FOR ISOLATION ***
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cloned_state = dict(rec)Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Execute the final node of the LangGraph flow
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = agent.execute_in_snowflake(cloned_state)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.execution_results.append(result)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_bar.progress((idx + 1) / total_recs)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  status_text.text("âœ… All executions complete!")
Â  Â  Â  Â  Â  Â  Â  Â  st.success("ðŸŽ‰ All queries attempted!")
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ Execution error: {str(e)}")
Â  Â Â 
Â  Â  if st.session_state.execution_results:
Â  Â  Â  Â  valid_results = [r for r in st.session_state.execution_results if r['new_execution_time_ms'] > 0 and r['original_execution_time_ms'] > 0]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- RESULTS METRICS ---
Â  Â  Â  Â  st.subheader("Summary Performance Metrics")
Â  Â  Â  Â  successful = len([r for r in st.session_state.execution_results if r['execution_status'] == 'SUCCESS'])
Â  Â  Â  Â Â 
Â  Â  Â  Â  col1, col2, col3, col4 = st.columns(4)
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  st.metric(" Successful Runs", f"{successful}/{len(st.session_state.execution_results)}")
Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  avg_improvement = sum([
Â  Â  Â  Â  Â  Â  Â  Â  ((r['original_execution_time_ms'] - r['new_execution_time_ms']) / r['original_execution_time_ms'] * 100)
Â  Â  Â  Â  Â  Â  Â  Â  for r in valid_results
Â  Â  Â  Â  Â  Â  ]) / max(len(valid_results), 1)
Â  Â  Â  Â  Â  Â  st.metric("âš¡ Avg Improvement", f"{avg_improvement:.1f}%")
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col3:
Â  Â  Â  Â  Â  Â  total_time_saved = sum([
Â  Â  Â  Â  Â  Â  Â  Â  (r['original_execution_time_ms'] - r['new_execution_time_ms'])
Â  Â  Â  Â  Â  Â  Â  Â  for r in st.session_state.execution_results
Â  Â  Â  Â  Â  Â  ])
Â  Â  Â  Â  Â  Â  st.metric("â±ï¸ Total Time Saved", f"{total_time_saved/1000:.1f}s")
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col4:
Â  Â  Â  Â  Â  Â  st.metric("ðŸ“Š Total Queries Analyzed", len(st.session_state.execution_results))
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- VISUALIZATION ---
Â  Â  Â  Â  if len(valid_results) > 0:
Â  Â  Â  Â  Â  Â  st.subheader("Execution Time Comparison")
Â  Â  Â  Â  Â  Â  fig = go.Figure()
Â  Â  Â  Â  Â  Â  query_ids = [r['query_id'][:8] for r in valid_results]
Â  Â  Â  Â  Â  Â  original_times = [r['original_execution_time_ms'] for r in valid_results]
Â  Â  Â  Â  Â  Â  new_times = [r['new_execution_time_ms'] for r in valid_results]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  fig.add_trace(go.Bar(name='Original Time', x=query_ids, y=original_times, marker_color='#ef4444'))
Â  Â  Â  Â  Â  Â  fig.add_trace(go.Bar(name='Optimized Time', x=query_ids, y=new_times, marker_color='#22c55e'))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  fig.update_layout(
Â  Â  Â  Â  Â  Â  Â  Â  title='Original vs. Optimized Execution Time (ms)',
Â  Â  Â  Â  Â  Â  Â  Â  xaxis_title='Query ID',
Â  Â  Â  Â  Â  Â  Â  Â  yaxis_title='Time (ms)',
Â  Â  Â  Â  Â  Â  Â  Â  barmode='group',
Â  Â  Â  Â  Â  Â  Â  Â  height=400
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  st.plotly_chart(fig, use_container_width=True)

Â  Â  Â  Â  # --- RESULTS DATAFRAME ---
Â  Â  Â  Â  st.subheader("Detailed Results Table")
Â  Â  Â  Â  results_df = pd.DataFrame([{
Â  Â  Â  Â  Â  Â  'Query ID': r['query_id'],
Â  Â  Â  Â  Â  Â  'Original WH': r['original_warehouse'],
Â  Â  Â  Â  Â  Â  'Recommended WH': r['recommended_warehouse'],
Â  Â  Â  Â  Â  Â  'Original Time (ms)': f"{r['original_execution_time_ms']:.0f}",
Â  Â  Â  Â  Â  Â  'New Time (ms)': f"{r['new_execution_time_ms']:.0f}",
Â  Â  Â  Â  Â  Â  'Improvement': f"{((r['original_execution_time_ms'] - r['new_execution_time_ms']) / r['original_execution_time_ms'] * 100):.1f}%" if r['new_execution_time_ms'] > 0 and r['original_execution_time_ms'] > 0 else 'N/A',
Â  Â  Â  Â  Â  Â  'Status': r['execution_status'],
Â  Â  Â  Â  Â  Â  'Result': r['execution_result']
Â  Â  Â  Â  } for r in st.session_state.execution_results])
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.dataframe(results_df, use_container_width=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Download results
Â  Â  Â  Â  csv = results_df.to_csv(index=False)
Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  label="ðŸ“¥ Download Results CSV",
Â  Â  Â  Â  Â  Â  data=csv,
Â  Â  Â  Â  Â  Â  file_name=f"warehouse_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
Â  Â  Â  Â  Â  Â  mime="text/csv"
Â  Â  Â  Â  )


# ==============================================================================
# MAIN EXECUTION ROUTER
# ==============================================================================

def main():
Â  Â  # Render the fixed, global header
Â  Â  st.markdown("""
Â  Â  <div class="main-header">
Â  Â  Â  Â  <h1>ðŸ¤– Snowflake Warehouse Optimizer with AI Agent</h1>
Â  Â  Â  Â  <p>Analyze queries, recommend warehouses, optimize SQL, and verify automatically using Gemini & LangGraph.</p>
Â  Â  </div>
Â  Â  """, unsafe_allow_html=True)

Â  Â  # --- FIX: INITIALIZE ALL SESSION STATE AT THE START ---
Â  Â  if 'page' not in st.session_state:
Â  Â  Â  Â  st.session_state.page = "data_load"
Â  Â  if 'all_records' not in st.session_state:
Â  Â  Â  Â  st.session_state.all_records = None
Â  Â  if 'selected_records' not in st.session_state:
Â  Â  Â  Â  st.session_state.selected_records = None
Â  Â  if 'recommendations' not in st.session_state:
Â  Â  Â  Â  st.session_state.recommendations = []
Â  Â  if 'execution_results' not in st.session_state:
Â  Â  Â  Â  st.session_state.execution_results = []
Â  Â Â 
Â  Â  # --- ROUTING LOGIC ---
Â  Â  if st.session_state.page == "data_load":
Â  Â  Â  Â  page_1_load_data()
Â  Â  elif st.session_state.page == "analysis":
Â  Â  Â  Â  # Check for necessary data before rendering page 2
Â  Â  Â  Â  if st.session_state.get('selected_records') is None:
Â  Â  Â  Â  Â  Â  set_page("data_load")
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  page_2_analysis()
Â  Â  elif st.session_state.page == "validation":
Â  Â  Â  Â  # Check for necessary data before rendering page 3
Â  Â  Â  Â  if not st.session_state.get('recommendations'):
Â  Â  Â  Â  Â  Â  set_page("analysis")
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  page_3_validation_and_results()

if __name__ == "__main__":
Â  Â  main()


